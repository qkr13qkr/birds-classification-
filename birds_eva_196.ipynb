{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/albumentations/__init__.py:13: UserWarning: A new version of Albumentations is available: 1.4.21 (you have 1.4.15). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import random # random\n",
    "import pandas as pd # pandas \n",
    "import numpy as np # numpy \n",
    "import cv2 # opencv\n",
    "import os \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "from albumentations.core.transforms_interface import ImageOnlyTransform #RobidouxSharp\n",
    "\n",
    "\n",
    "import timm\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch.transforms import ToTensorV2\n",
    "import torchvision.models as models\n",
    "import ttach as tta\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "CFG = {\n",
    "    \"IMG_SIZE\" : 196,\n",
    "    \"RSR_IMG_SIZE\" : 1024,\n",
    "    \"EPOCHS\" : 100,\n",
    "    \"LEARNING_RATE\" : 1e-4,\n",
    "    \"BATCH_SIZE\" : 256,\n",
    "    \"SEED\" : 42,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix seed\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataload\n",
    "train_df = pd.read_csv(\"./train.csv\")\n",
    "test_df = pd.read_csv(\"./test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_img_path fix \n",
    "path = \"./train/\"\n",
    "train_df[\"img_path\"] = train_df[\"img_path\"].apply(lambda x : path + x.split(\"/\")[-1])\n",
    "\n",
    "path = \"./up_train/\"\n",
    "train_df[\"upscale_img_path\"] = train_df[\"upscale_img_path\"].apply(lambda x : path + x.split(\"/\")[-1])\n",
    "\n",
    "\n",
    "path = \"./test/\"\n",
    "test_df[\"img_path\"] = test_df[\"img_path\"].apply(lambda x : path + x.split(\"/\")[-1])\n",
    "\n",
    "path = \"./up_test/\"\n",
    "test_df[\"upscale_img_path\"] = test_df[\"img_path\"].apply(lambda x : path + x.split(\"/\")[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df[\"label\"])\n",
    "train_df[\"label\"] = le.transform(train_df[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 0:\n",
      "  Train: index=[    0     2     3 ... 15830 15831 15833]\n",
      "  Test:  index=[    1     4     5 ... 15808 15811 15832]\n",
      "Fold 1:\n",
      "  Train: index=[    0     1     2 ... 15830 15832 15833]\n",
      "  Test:  index=[    6     8    13 ... 15825 15826 15831]\n",
      "Fold 2:\n",
      "  Train: index=[    1     3     4 ... 15831 15832 15833]\n",
      "  Test:  index=[    0     2    15 ... 15810 15824 15830]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "skf = StratifiedKFold(n_splits=5 , shuffle= True , random_state= CFG[\"SEED\"])\n",
    "for i, (train_index, test_index) in enumerate(skf.split(train_df[\"img_path\"], train_df[\"label\"])):\n",
    "     print(f\"Fold {i}:\")\n",
    "     print(f\"  Train: index={train_index}\")\n",
    "     print(f\"  Test:  index={test_index}\")\n",
    "     \n",
    "     if i == 2:\n",
    "          break\n",
    "val_df = train_df.iloc[test_index]\n",
    "train_df = train_df.iloc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, img_path_list, label_list, transforms=None , flag_mixup = False):\n",
    "        self.img_path_list = img_path_list\n",
    "        self.label_list = label_list\n",
    "        self.transforms = transforms\n",
    "        self.flag_mixup = flag_mixup\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        img_path = self.img_path_list[index]\n",
    "        image = cv2.imread(img_path)\n",
    "        if self.label_list is not None:\n",
    "            label = torch.zeros(len(le.classes_))\n",
    "            label[self.label_list[index]] = 1.\n",
    "            \n",
    "        # 기존 image data\n",
    "        if self.transforms is not None:\n",
    "            image = self.transforms(image=image)['image']\n",
    "        \n",
    "        if self.flag_mixup :\n",
    "            # mixup에 사용될 data 선택\n",
    "            mixup_label = torch.zeros(len(le.classes_))\n",
    "\n",
    "            while True:\n",
    "              mixup_idx = random.randint(0, len(self.img_path_list)-1) # 전체 데이터 중 아무거나 선택 / 중복되는 클래스가 선택될 수 있음\n",
    "              if self.label_list[mixup_idx] != self.label_list[index]: # 같은 카테고리 방지\n",
    "                mixup_label[self.label_list[mixup_idx]] = 1.\n",
    "                break\n",
    "        \n",
    "            # mix할 이미지\n",
    "            mixup_image = cv2.imread(self.img_path_list[mixup_idx])\n",
    "            if self.transforms is not None:\n",
    "                mixup_image = self.transforms(image = mixup_image)[\"image\"]\n",
    "\n",
    "            # Select a random number from the given beta distribution\n",
    "            # Mixup the images accordingly\n",
    "            alpha = 0.4\n",
    "            lam = np.random.beta(alpha, alpha)\n",
    "            image = lam * image + (1 - lam) * mixup_image\n",
    "            label = lam * label + (1 - lam) * mixup_label\n",
    "\n",
    "        # label one-hot으로 생성\n",
    "        if self.label_list is not None:\n",
    "            return image, label\n",
    "        else:\n",
    "            return image\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_path_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom RobidouxSharp resize transform\n",
    "class RobidouxSharpResize(ImageOnlyTransform):\n",
    "    def __init__(self, height, width, always_apply=False, p=1.0):\n",
    "        super(RobidouxSharpResize, self).__init__(always_apply, p)\n",
    "        self.height = height\n",
    "        self.width = width\n",
    "\n",
    "    def apply(self, img, **params):\n",
    "        return cv2.resize(img, (self.width, self.height), interpolation=cv2.INTER_CUBIC)  # Approximating RobidouxSharp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = A.Compose([\n",
    "    RobidouxSharpResize(CFG['RSR_IMG_SIZE'], CFG['RSR_IMG_SIZE']),\n",
    "    A.Resize(CFG[\"IMG_SIZE\"], CFG[\"IMG_SIZE\"], interpolation=2),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    \n",
    "   # 배경을 흐리게 하여 객체를 강조하는 효과 추가\n",
    "    A.OneOf([\n",
    "        A.GaussNoise(var_limit=(10.0, 50.0), p=0.15),  # 잡음을 너무 과하게 넣지 않기 위해 확률과 강도를 줄임\n",
    "        A.GaussianBlur(blur_limit=(3, 5), p=0.2),  # 블러 강도를 줄여 배경 흐림 처리만 약간 적용\n",
    "        A.MotionBlur(blur_limit=3, p=0.2),\n",
    "    ], p=0.4),  # 전체적인 적용 확률을 낮춰서 원본 이미지 손상을 줄임\n",
    "    \n",
    "    # 객체 강조를 위한 추가 전처리\n",
    "    A.OneOf([\n",
    "        A.CLAHE(clip_limit=1.5, p=0.2),  # 대비를 너무 과하게 적용하지 않도록 조정\n",
    "        A.RandomBrightnessContrast(brightness_limit=0.1, contrast_limit=0.1, p=0.2),  # 밝기와 대비를 적당히 조절하여 이미지 특징 유지\n",
    "        A.Sharpen(alpha=(0.1, 0.3), lightness=(0.5, 1.0), p=0.2)  # 샤프닝 효과를 너무 강하지 않게 적용\n",
    "    ], p=0.4),  # 전체적인 적용 확률을 낮춤\n",
    "\n",
    "\n",
    "    A.Rotate(limit=15, p=0.4),\n",
    "    \n",
    "    # 선택적으로 배경을 흐리게 하여 조류를 강조\n",
    "    A.OneOf([\n",
    "        A.Blur(blur_limit=3, p=0.2),  # 블러 강도를 낮춰 원본 이미지의 세부 사항 보존\n",
    "        A.ImageCompression(quality_lower=70, quality_upper=90, p=0.2),  # 압축 강도를 적당히 하여 특징을 유지\n",
    "    ], p=0.3),  # 전체적인 확률을 조정하여 중요한 세부 사항 보존\n",
    "    \n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "    ToTensorV2()\n",
    "])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(CFG[\"IMG_SIZE\"],CFG[\"IMG_SIZE\"],interpolation=2),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntrain_transform = A.Compose([\\n                            RobidouxSharpResize(CFG[\\'RSR_IMG_SIZE\\'], CFG[\\'RSR_IMG_SIZE\\']),\\n                            A.Resize(CFG[\"IMG_SIZE\"],CFG[\"IMG_SIZE\"],interpolation=2),\\n                            A.HorizontalFlip(p= 0.5), \\n                            A.OneOf([\\n                                    A.GaussNoise(p=0.2),\\n                                    A.GaussianBlur(p=0.2),\\n                                    A.MotionBlur(p=0.2),\\n                            ], p=0.5),\\n                            A.Rotate(limit =30 , p = 0.5),\\n                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\\n                            ToTensorV2()\\n                            ])\\n\\ntest_transform = A.Compose([\\n                            A.Resize(CFG[\"IMG_SIZE\"],CFG[\"IMG_SIZE\"],interpolation=2),\\n                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\\n                            ToTensorV2()\\n                            ])\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "train_transform = A.Compose([\n",
    "                            RobidouxSharpResize(CFG['RSR_IMG_SIZE'], CFG['RSR_IMG_SIZE']),\n",
    "                            A.Resize(CFG[\"IMG_SIZE\"],CFG[\"IMG_SIZE\"],interpolation=2),\n",
    "                            A.HorizontalFlip(p= 0.5), \n",
    "                            A.OneOf([\n",
    "                                    A.GaussNoise(p=0.2),\n",
    "                                    A.GaussianBlur(p=0.2),\n",
    "                                    A.MotionBlur(p=0.2),\n",
    "                            ], p=0.5),\n",
    "                            A.Rotate(limit =30 , p = 0.5),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "\n",
    "test_transform = A.Compose([\n",
    "                            A.Resize(CFG[\"IMG_SIZE\"],CFG[\"IMG_SIZE\"],interpolation=2),\n",
    "                            A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225), max_pixel_value=255.0, p=1.0),\n",
    "                            ToTensorV2()\n",
    "                            ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"upscale_img_path\"] = train_df[\"upscale_img_path\"].apply(lambda x : x.replace(\"png\" , \"jpg\"))\n",
    "val_df[\"upscale_img_path\"] = val_df[\"upscale_img_path\"].apply(lambda x : x.replace(\"png\" , \"jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataset = CustomDataset(train_df['img_path'].values, train_df['label'].values, train_transform , flag_mixup = True )\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=4)\n",
    "\n",
    "val_dataset = CustomDataset(val_df['img_path'].values, val_df['label'].values, test_transform)\n",
    "val_loader = DataLoader(val_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, num_classes=len(le.classes_)):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.backbone = timm.create_model('eva_large_patch14_196.in22k_ft_in22k_in1k', pretrained=True).to(device)\n",
    "        self.classifier = nn.Linear(1000, num_classes)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for imgs, labels in tqdm(iter(train_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(imgs)\n",
    "            loss = criterion(output, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "                    \n",
    "        _val_loss, _val_score = validation(model, criterion, val_loader, device)\n",
    "        _train_loss = np.mean(train_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{_train_loss:.5f}] Val Loss : [{_val_loss:.5f}] Val Macro F1 Score : [{_val_score:.5f}]')\n",
    "       \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(_val_score)\n",
    "            \n",
    "        if best_score < _val_score:\n",
    "            best_score = _val_score\n",
    "            best_model = model\n",
    "            print(\"model save\")\n",
    "            torch.save(best_model.state_dict() , \"./models/eva_196_64*64_scale_best2.pt\")\n",
    "        torch.save(model.state_dict() , \"./models/eva_196_64*64_scale_last2.pt\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, criterion, val_loader, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    preds, true_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(iter(val_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            \n",
    "            loss = criterion(pred, labels)\n",
    "            \n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            true_labels += labels.argmax(1).detach().cpu().numpy().tolist()\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "        \n",
    "        _val_loss = np.mean(val_loss)\n",
    "        _val_score = f1_score(true_labels, preds, average='macro')\n",
    "        \n",
    "    \n",
    "    return _val_loss, _val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "570f19651ceb4775bcba88af58500672",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1552394/1230697910.py\", line 8, in forward\n    x = self.backbone(x)\n        ^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/timm/models/vision_transformer.py\", line 829, in forward\n    x = self.forward_features(x)\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/timm/models/vision_transformer.py\", line 810, in forward_features\n    x = self.blocks(x)\n        ^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/timm/models/vision_transformer.py\", line 166, in forward\n    x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n                                     ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/timm/layers/mlp.py\", line 43, in forward\n    x = self.act(x)\n        ^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 705, in forward\n    return F.gelu(input, approximate=self.approximate)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 232.00 MiB is free. Process 1521096 has 17.88 GiB memory in use. Including non-PyTorch memory, this process has 29.28 GiB memory in use. Of the allocated memory 28.62 GiB is allocated by PyTorch, and 233.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(params \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mparameters(), lr \u001b[38;5;241m=\u001b[39m CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLEARNING_RATE\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      5\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, threshold_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mabs\u001b[39m\u001b[38;5;124m'\u001b[39m, min_lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-8\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 7\u001b[0m infer_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 17\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, train_loader, val_loader, scheduler, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     15\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 17\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output, labels)\n\u001b[1;32m     20\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:186\u001b[0m, in \u001b[0;36mDataParallel.forward\u001b[0;34m(self, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule(\u001b[38;5;241m*\u001b[39minputs[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodule_kwargs[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    185\u001b[0m replicas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplicate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodule, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice_ids[:\u001b[38;5;28mlen\u001b[39m(inputs)])\n\u001b[0;32m--> 186\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodule_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgather(outputs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/parallel/data_parallel.py:201\u001b[0m, in \u001b[0;36mDataParallel.parallel_apply\u001b[0;34m(self, replicas, inputs, kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparallel_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, replicas: Sequence[T], inputs: Sequence[Any], kwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Any]:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mparallel_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_ids\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mreplicas\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py:109\u001b[0m, in \u001b[0;36mparallel_apply\u001b[0;34m(modules, inputs, kwargs_tup, devices)\u001b[0m\n\u001b[1;32m    107\u001b[0m     output \u001b[38;5;241m=\u001b[39m results[i]\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(output, ExceptionWrapper):\n\u001b[0;32m--> 109\u001b[0m         \u001b[43moutput\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(output)\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/anaconda3/envs/test/lib/python3.11/site-packages/torch/_utils.py:706\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 706\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: Caught OutOfMemoryError in replica 0 on device 0.\nOriginal Traceback (most recent call last):\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/parallel/parallel_apply.py\", line 84, in _worker\n    output = module(*input, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/tmp/ipykernel_1552394/1230697910.py\", line 8, in forward\n    x = self.backbone(x)\n        ^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/timm/models/vision_transformer.py\", line 829, in forward\n    x = self.forward_features(x)\n        ^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/timm/models/vision_transformer.py\", line 810, in forward_features\n    x = self.blocks(x)\n        ^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/container.py\", line 219, in forward\n    input = module(input)\n            ^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/timm/models/vision_transformer.py\", line 166, in forward\n    x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))\n                                     ^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/timm/layers/mlp.py\", line 43, in forward\n    x = self.act(x)\n        ^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1553, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/module.py\", line 1562, in _call_impl\n    return forward_call(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/kwy00/anaconda3/envs/test/lib/python3.11/site-packages/torch/nn/modules/activation.py\", line 705, in forward\n    return F.gelu(input, approximate=self.approximate)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\ntorch.OutOfMemoryError: CUDA out of memory. Tried to allocate 394.00 MiB. GPU 0 has a total capacity of 47.43 GiB of which 232.00 MiB is free. Process 1521096 has 17.88 GiB memory in use. Including non-PyTorch memory, this process has 29.28 GiB memory in use. Of the allocated memory 28.62 GiB is allocated by PyTorch, and 233.60 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model = torch.nn.DataParallel(model).to(device)\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, threshold_mode='abs', min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = BaseModel().to(device)\n",
    "load_model = torch.nn.DataParallel(load_model)\n",
    "load_model.load_state_dict(torch.load('./models/eva_196_64*64_scale_best.pt'))\n",
    "load_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ttach as tta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_df['img_path'].values, None, test_transform)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'] * 10, shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>img_path</th>\n",
       "      <th>upscale_img_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>./test/TEST_00000.jpg</td>\n",
       "      <td>./up_test/TEST_00000.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>./test/TEST_00001.jpg</td>\n",
       "      <td>./up_test/TEST_00001.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>./test/TEST_00002.jpg</td>\n",
       "      <td>./up_test/TEST_00002.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>./test/TEST_00003.jpg</td>\n",
       "      <td>./up_test/TEST_00003.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>./test/TEST_00004.jpg</td>\n",
       "      <td>./up_test/TEST_00004.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6781</th>\n",
       "      <td>TEST_06781</td>\n",
       "      <td>./test/TEST_06781.jpg</td>\n",
       "      <td>./up_test/TEST_06781.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6782</th>\n",
       "      <td>TEST_06782</td>\n",
       "      <td>./test/TEST_06782.jpg</td>\n",
       "      <td>./up_test/TEST_06782.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6783</th>\n",
       "      <td>TEST_06783</td>\n",
       "      <td>./test/TEST_06783.jpg</td>\n",
       "      <td>./up_test/TEST_06783.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6784</th>\n",
       "      <td>TEST_06784</td>\n",
       "      <td>./test/TEST_06784.jpg</td>\n",
       "      <td>./up_test/TEST_06784.jpg</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6785</th>\n",
       "      <td>TEST_06785</td>\n",
       "      <td>./test/TEST_06785.jpg</td>\n",
       "      <td>./up_test/TEST_06785.jpg</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6786 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              id               img_path          upscale_img_path\n",
       "0     TEST_00000  ./test/TEST_00000.jpg  ./up_test/TEST_00000.jpg\n",
       "1     TEST_00001  ./test/TEST_00001.jpg  ./up_test/TEST_00001.jpg\n",
       "2     TEST_00002  ./test/TEST_00002.jpg  ./up_test/TEST_00002.jpg\n",
       "3     TEST_00003  ./test/TEST_00003.jpg  ./up_test/TEST_00003.jpg\n",
       "4     TEST_00004  ./test/TEST_00004.jpg  ./up_test/TEST_00004.jpg\n",
       "...          ...                    ...                       ...\n",
       "6781  TEST_06781  ./test/TEST_06781.jpg  ./up_test/TEST_06781.jpg\n",
       "6782  TEST_06782  ./test/TEST_06782.jpg  ./up_test/TEST_06782.jpg\n",
       "6783  TEST_06783  ./test/TEST_06783.jpg  ./up_test/TEST_06783.jpg\n",
       "6784  TEST_06784  ./test/TEST_06784.jpg  ./up_test/TEST_06784.jpg\n",
       "6785  TEST_06785  ./test/TEST_06785.jpg  ./up_test/TEST_06785.jpg\n",
       "\n",
       "[6786 rows x 3 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    i = 0\n",
    "    with torch.no_grad():\n",
    "        for imgs in tqdm(iter(test_loader)):\n",
    "            imgs = imgs.float().to(device)\n",
    "            \n",
    "            pred = model(imgs)\n",
    "            # print(pred)\n",
    "            preds += pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "            # preds += pred.detach().cpu()\n",
    "    preds = le.inverse_transform(preds)\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [02:31<00:00, 21.60s/it]\n"
     ]
    }
   ],
   "source": [
    "tta_ = tta.Compose(\n",
    "    [\n",
    "      tta.HorizontalFlip(),\n",
    "      tta.Multiply(factors=[0.9, 1, 1.1])\n",
    "    ]\n",
    ")\n",
    "tta_model = tta.ClassificationTTAWrapper(load_model, tta_)\n",
    "preds = inference(tta_model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('./sample_submission.csv')\n",
    "submission['label'] = preds\n",
    "submission.to_csv('./0504-eva_196_64*64.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
